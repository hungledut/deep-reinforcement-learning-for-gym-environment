{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89072195-9b83-4bac-b3e8-028ebf4508ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f3ad98-653e-444c-9d51-35e2a4102d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb3f79-36b7-4c3b-a9df-3139b8fb7a70",
   "metadata": {},
   "source": [
    "### Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2ade44-9fee-4b3c-8b07-0ad1cf9386b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space: 8\n",
      "Action Space: 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print('State Space:', state_space)\n",
    "action_space = env.action_space.n\n",
    "print('Action Space:', action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b26a4bde-bb92-4668-bba2-e637af58a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self , s_size , a_size , h_size ):\n",
    "        super (Policy , self ).__init__ ()\n",
    "        self.fc1 = nn.Linear( s_size , h_size )\n",
    "        self.fc2 = nn.Linear( h_size , h_size * 2)\n",
    "        self.fc3 = nn.Linear( h_size * 2, a_size )\n",
    "    def forward(self , x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim =1)\n",
    "    def act(self, state ):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)  #.to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        # Random action\n",
    "        action = m.sample()\n",
    "        return action.item() , m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452031f-a4ae-4974-8cdc-cad4dcbb9c5b",
   "metadata": {},
   "source": [
    "### REINFORCE \n",
    "Initialize the policy parameter $\\theta$ at random. <br>\n",
    "**for** each episode $\\{s_1, a_1, r_2, s_2, a_2, \\dots, s_T\\} $ **do** <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;     **for** $t=1, 2, … , T $ **do** <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Calculate the Return $G_t$ <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Update policy parameters $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi_\\theta(A_t \\vert S_t)$ <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; **end for** <br>\n",
    "**end for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffb2409c-f6ce-4705-ae91-eae3f41a9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def reinforce(\n",
    "        policy ,\n",
    "        optimizer ,\n",
    "        n_training_episodes ,\n",
    "        max_steps ,\n",
    "        gamma ,\n",
    "        print_every\n",
    "        ):\n",
    "    scores_deque = deque( maxlen =100)\n",
    "    scores = []\n",
    "\n",
    "    # Each Episode\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # t=1, 2, … , T\n",
    "        for t in range(max_steps):\n",
    "            action , log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state , reward , done , _, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done :\n",
    "                break\n",
    "        scores_deque.append(sum( rewards ))\n",
    "        scores.append(sum( rewards ))\n",
    "\n",
    "        returns = deque( maxlen = max_steps )\n",
    "        n_steps = len( rewards )\n",
    "\n",
    "        # List of discounted Returns\n",
    "        for t in range(n_steps)[:: -1]:\n",
    "            disc_return_t = returns[0] if len( returns ) > 0 else 0\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t])\n",
    "\n",
    "        eps = np.finfo(np.float32 ).eps.item()\n",
    "\n",
    "        returns = torch.tensor( returns )\n",
    "        returns = ( returns - returns.mean()) / ( returns.std() + eps)\n",
    "\n",
    "        # Total loss\n",
    "        policy_loss = []\n",
    "        for log_prob , disc_return in zip( saved_log_probs , returns ):\n",
    "            policy_loss.append(-log_prob * disc_return )\n",
    "        policy_loss = torch.cat( policy_loss ).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print(\" Episode {}\\ tAverage Score : {:.2 f}\".format( i_episode , np.mean(scores_deque )))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39a5ed0d-52c1-4a61-97e3-19ceb32b2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy (\n",
    "        s_size = state_space ,\n",
    "        a_size = action_space ,\n",
    "        h_size = h_size ,\n",
    "        ).to( device )\n",
    "optimizer = optim.Adam( policy.parameters() , lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6d77bd6-5277-4100-bafe-ad8769374364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "h_size = 128\n",
    "lr = 0.001\n",
    "n_training_episodes = 20\n",
    "max_steps = 10\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1eebf12-e226-47a9-b16f-4207edd8860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = reinforce (\n",
    "        policy ,\n",
    "        optimizer ,\n",
    "        n_training_episodes ,\n",
    "        max_steps ,\n",
    "        gamma ,\n",
    "        print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687aecd8-1dd1-4282-806d-9c3c42837982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea15a2-1eba-4cc9-abca-b8b6dcbd9a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
