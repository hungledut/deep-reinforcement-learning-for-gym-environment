{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Z8mnQcETNdk7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8mnQcETNdk7",
    "outputId": "15c2c95d-345e-44dc-ce38-fc156ac5421d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517a71c8-601d-4e3e-9212-a9ad82ab4f94",
   "metadata": {
    "id": "517a71c8-601d-4e3e-9212-a9ad82ab4f94"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3ce56d-eb25-4407-9488-a2c392f70197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee3ce56d-eb25-4407-9488-a2c392f70197",
    "outputId": "6dba4ca9-9c0e-4d50-f678-a1e9ea483ec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d22d231-e34b-457c-9feb-00056b6cbf1f",
   "metadata": {
    "id": "8d22d231-e34b-457c-9feb-00056b6cbf1f"
   },
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        # ACTOR\n",
    "        if has_continuous_action_space : # continuous action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else: # discrete action space\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "\n",
    "        # CRITIC\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        # CRITIC\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        # ACTOR\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        # CRITIC\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wkFuZ_koWX8g",
   "metadata": {
    "id": "wkFuZ_koWX8g"
   },
   "source": [
    "### Proximal Policy Optimization\n",
    "Link: https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914615f8-5a1f-4f2d-afa4-84a9ada2c63c",
   "metadata": {
    "id": "914615f8-5a1f-4f2d-afa4-84a9ada2c63c"
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bce4f3-4755-45e8-a0ad-9512006a9138",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93bce4f3-4755-45e8-a0ad-9512006a9138",
    "outputId": "a74c259b-9e46-44a0-96df-4fd2032a3155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training environment name : CartPole-v1\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  500\n",
      "log frequency : 1000 timesteps\n",
      "printing average reward over episodes in last : 2000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  4\n",
      "action space dimension :  2\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a discrete action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 2000 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Episode  0 : Reward =  23.0\n",
      "Episode  1 : Reward =  38.0\n",
      "Episode  2 : Reward =  19.0\n",
      "Episode  3 : Reward =  21.0\n",
      "Episode  4 : Reward =  15.0\n",
      "Episode  5 : Reward =  18.0\n",
      "Episode  6 : Reward =  27.0\n",
      "Episode  7 : Reward =  43.0\n",
      "Episode  8 : Reward =  11.0\n",
      "Episode  9 : Reward =  12.0\n",
      "Episode  10 : Reward =  14.0\n",
      "Episode  11 : Reward =  37.0\n",
      "Episode  12 : Reward =  13.0\n",
      "Episode  13 : Reward =  17.0\n",
      "Episode  14 : Reward =  12.0\n",
      "Episode  15 : Reward =  16.0\n",
      "Episode  16 : Reward =  17.0\n",
      "Episode  17 : Reward =  12.0\n",
      "Episode  18 : Reward =  41.0\n",
      "Episode  19 : Reward =  43.0\n",
      "Episode  20 : Reward =  21.0\n",
      "Episode  21 : Reward =  15.0\n",
      "Episode  22 : Reward =  12.0\n",
      "Episode  23 : Reward =  20.0\n",
      "Episode  24 : Reward =  28.0\n",
      "Episode  25 : Reward =  16.0\n",
      "Episode  26 : Reward =  17.0\n",
      "Episode  27 : Reward =  11.0\n",
      "Episode  28 : Reward =  24.0\n",
      "Episode  29 : Reward =  9.0\n",
      "Episode  30 : Reward =  15.0\n",
      "Episode  31 : Reward =  22.0\n",
      "Episode  32 : Reward =  12.0\n",
      "Episode  33 : Reward =  27.0\n",
      "Episode  34 : Reward =  11.0\n",
      "Episode  35 : Reward =  15.0\n",
      "Episode  36 : Reward =  27.0\n",
      "Episode  37 : Reward =  54.0\n",
      "Episode  38 : Reward =  26.0\n",
      "Episode  39 : Reward =  43.0\n",
      "Episode  40 : Reward =  21.0\n",
      "Episode  41 : Reward =  32.0\n",
      "Episode  42 : Reward =  18.0\n",
      "Episode  43 : Reward =  59.0\n",
      "Episode  44 : Reward =  21.0\n",
      "Episode  45 : Reward =  20.0\n",
      "Episode  46 : Reward =  31.0\n",
      "Episode  47 : Reward =  13.0\n",
      "Episode  48 : Reward =  34.0\n",
      "Episode  49 : Reward =  23.0\n",
      "Episode  50 : Reward =  73.0\n",
      "Episode  51 : Reward =  13.0\n",
      "Episode  52 : Reward =  29.0\n",
      "Episode  53 : Reward =  18.0\n",
      "Episode  54 : Reward =  45.0\n",
      "Episode  55 : Reward =  15.0\n",
      "Episode  56 : Reward =  20.0\n",
      "Episode  57 : Reward =  16.0\n",
      "Episode  58 : Reward =  12.0\n",
      "Episode  59 : Reward =  11.0\n",
      "Episode  60 : Reward =  14.0\n",
      "Episode  61 : Reward =  11.0\n",
      "Episode  62 : Reward =  18.0\n",
      "Episode  63 : Reward =  24.0\n",
      "Episode  64 : Reward =  32.0\n",
      "Episode  65 : Reward =  13.0\n",
      "Episode  66 : Reward =  16.0\n",
      "Episode  67 : Reward =  36.0\n",
      "Episode  68 : Reward =  13.0\n",
      "Episode  69 : Reward =  38.0\n",
      "Episode  70 : Reward =  14.0\n",
      "Episode  71 : Reward =  20.0\n",
      "Episode  72 : Reward =  39.0\n",
      "Episode  73 : Reward =  20.0\n",
      "Episode  74 : Reward =  10.0\n",
      "Episode  75 : Reward =  11.0\n",
      "Episode  76 : Reward =  24.0\n",
      "Episode  77 : Reward =  10.0\n",
      "Episode  78 : Reward =  25.0\n",
      "Episode  79 : Reward =  26.0\n",
      "Episode  80 : Reward =  26.0\n",
      "Episode  81 : Reward =  26.0\n",
      "Episode  82 : Reward =  22.0\n",
      "Episode  83 : Reward =  68.0\n",
      "Episode  84 : Reward =  16.0\n",
      "Episode  85 : Reward =  27.0\n",
      "Episode  86 : Reward =  13.0\n",
      "Episode  87 : Reward =  23.0\n",
      "Episode  88 : Reward =  16.0\n",
      "Episode  89 : Reward =  15.0\n",
      "Episode  90 : Reward =  12.0\n",
      "Episode  91 : Reward =  33.0\n",
      "Episode  92 : Reward =  43.0\n",
      "Episode  93 : Reward =  21.0\n",
      "Episode  94 : Reward =  31.0\n",
      "Episode  95 : Reward =  28.0\n",
      "Episode  96 : Reward =  26.0\n",
      "Episode  97 : Reward =  13.0\n",
      "Episode  98 : Reward =  25.0\n",
      "Episode  99 : Reward =  19.0\n",
      "Episode  100 : Reward =  20.0\n",
      "Episode  101 : Reward =  74.0\n",
      "Episode  102 : Reward =  16.0\n",
      "Episode  103 : Reward =  19.0\n",
      "Episode  104 : Reward =  25.0\n",
      "Episode  105 : Reward =  48.0\n",
      "Episode  106 : Reward =  11.0\n",
      "Episode  107 : Reward =  10.0\n",
      "Episode  108 : Reward =  27.0\n",
      "Episode  109 : Reward =  15.0\n",
      "Episode  110 : Reward =  12.0\n",
      "Episode  111 : Reward =  16.0\n",
      "Episode  112 : Reward =  16.0\n",
      "Episode  113 : Reward =  21.0\n",
      "Episode  114 : Reward =  17.0\n",
      "Episode  115 : Reward =  18.0\n",
      "Episode  116 : Reward =  15.0\n",
      "Episode  117 : Reward =  20.0\n",
      "Episode  118 : Reward =  14.0\n",
      "Episode  119 : Reward =  25.0\n",
      "Episode  120 : Reward =  46.0\n",
      "Episode  121 : Reward =  20.0\n",
      "Episode  122 : Reward =  40.0\n",
      "Episode  123 : Reward =  51.0\n",
      "Episode  124 : Reward =  26.0\n",
      "Episode  125 : Reward =  24.0\n",
      "Episode  126 : Reward =  30.0\n",
      "Episode  127 : Reward =  12.0\n",
      "Episode  128 : Reward =  17.0\n",
      "Episode  129 : Reward =  31.0\n",
      "Episode  130 : Reward =  25.0\n",
      "Episode  131 : Reward =  43.0\n",
      "Episode  132 : Reward =  19.0\n",
      "Episode  133 : Reward =  21.0\n",
      "Episode  134 : Reward =  40.0\n",
      "Episode  135 : Reward =  17.0\n",
      "Episode  136 : Reward =  33.0\n",
      "Episode  137 : Reward =  18.0\n",
      "Episode  138 : Reward =  32.0\n",
      "Episode  139 : Reward =  12.0\n",
      "Episode  140 : Reward =  34.0\n",
      "Episode  141 : Reward =  11.0\n",
      "Episode  142 : Reward =  12.0\n",
      "Episode  143 : Reward =  24.0\n",
      "Episode  144 : Reward =  36.0\n",
      "Episode  145 : Reward =  44.0\n",
      "Episode  146 : Reward =  40.0\n",
      "Episode  147 : Reward =  62.0\n",
      "Episode  148 : Reward =  38.0\n",
      "Episode  149 : Reward =  31.0\n",
      "Episode  150 : Reward =  41.0\n",
      "Episode  151 : Reward =  31.0\n",
      "Episode  152 : Reward =  20.0\n",
      "Episode  153 : Reward =  35.0\n",
      "Episode  154 : Reward =  40.0\n",
      "Episode  155 : Reward =  22.0\n",
      "Episode  156 : Reward =  65.0\n",
      "Episode  157 : Reward =  27.0\n",
      "Episode  158 : Reward =  63.0\n",
      "Episode  159 : Reward =  13.0\n",
      "Episode  160 : Reward =  15.0\n",
      "Episode  161 : Reward =  34.0\n",
      "Episode  162 : Reward =  21.0\n",
      "Episode  163 : Reward =  50.0\n",
      "Episode  164 : Reward =  18.0\n",
      "Episode  165 : Reward =  34.0\n",
      "Episode  166 : Reward =  14.0\n",
      "Episode  167 : Reward =  54.0\n",
      "Episode  168 : Reward =  75.0\n",
      "Episode  169 : Reward =  34.0\n",
      "Episode  170 : Reward =  16.0\n",
      "Episode  171 : Reward =  51.0\n",
      "Episode  172 : Reward =  52.0\n",
      "Episode  173 : Reward =  18.0\n",
      "Episode  174 : Reward =  82.0\n",
      "Episode  175 : Reward =  29.0\n",
      "Episode  176 : Reward =  27.0\n",
      "Episode  177 : Reward =  26.0\n",
      "Episode  178 : Reward =  18.0\n",
      "Episode  179 : Reward =  15.0\n",
      "Episode  180 : Reward =  37.0\n",
      "Episode  181 : Reward =  21.0\n",
      "Episode  182 : Reward =  94.0\n",
      "Episode  183 : Reward =  30.0\n",
      "Episode  184 : Reward =  30.0\n",
      "Episode  185 : Reward =  18.0\n",
      "Episode  186 : Reward =  23.0\n",
      "Episode  187 : Reward =  58.0\n",
      "Episode  188 : Reward =  31.0\n",
      "Episode  189 : Reward =  25.0\n",
      "Episode  190 : Reward =  14.0\n",
      "Episode  191 : Reward =  14.0\n",
      "Episode  192 : Reward =  22.0\n",
      "Episode  193 : Reward =  30.0\n",
      "Episode  194 : Reward =  33.0\n",
      "Episode  195 : Reward =  13.0\n",
      "Episode  196 : Reward =  35.0\n",
      "Episode  197 : Reward =  10.0\n",
      "Episode  198 : Reward =  48.0\n",
      "Episode  199 : Reward =  18.0\n",
      "Episode  200 : Reward =  11.0\n",
      "Episode  201 : Reward =  37.0\n",
      "Episode  202 : Reward =  24.0\n",
      "Episode  203 : Reward =  20.0\n",
      "Episode  204 : Reward =  12.0\n",
      "Episode  205 : Reward =  20.0\n",
      "Episode  206 : Reward =  16.0\n",
      "Episode  207 : Reward =  13.0\n",
      "Episode  208 : Reward =  18.0\n",
      "Episode  209 : Reward =  117.0\n",
      "Episode  210 : Reward =  34.0\n",
      "Episode  211 : Reward =  47.0\n",
      "Episode  212 : Reward =  20.0\n",
      "Episode  213 : Reward =  71.0\n",
      "Episode  214 : Reward =  39.0\n",
      "Episode  215 : Reward =  25.0\n",
      "Episode  216 : Reward =  20.0\n",
      "Episode  217 : Reward =  33.0\n",
      "Episode  218 : Reward =  17.0\n",
      "Episode  219 : Reward =  63.0\n",
      "Episode  220 : Reward =  15.0\n",
      "Episode  221 : Reward =  49.0\n",
      "Episode  222 : Reward =  33.0\n",
      "Episode  223 : Reward =  98.0\n",
      "Episode  224 : Reward =  75.0\n",
      "Episode  225 : Reward =  21.0\n",
      "Episode  226 : Reward =  99.0\n",
      "Episode  227 : Reward =  30.0\n",
      "Episode  228 : Reward =  74.0\n",
      "Episode  229 : Reward =  57.0\n",
      "Episode  230 : Reward =  112.0\n",
      "Episode  231 : Reward =  36.0\n",
      "Episode  232 : Reward =  76.0\n",
      "Episode  233 : Reward =  74.0\n",
      "Episode  234 : Reward =  90.0\n",
      "Episode  235 : Reward =  16.0\n",
      "Episode  236 : Reward =  26.0\n",
      "Episode  237 : Reward =  55.0\n",
      "Episode  238 : Reward =  17.0\n",
      "Episode  239 : Reward =  16.0\n",
      "Episode  240 : Reward =  69.0\n",
      "Episode  241 : Reward =  22.0\n",
      "Episode  242 : Reward =  50.0\n",
      "Episode  243 : Reward =  68.0\n",
      "Episode  244 : Reward =  22.0\n",
      "Episode  245 : Reward =  20.0\n",
      "Episode  246 : Reward =  46.0\n",
      "Episode  247 : Reward =  68.0\n",
      "Episode  248 : Reward =  21.0\n",
      "Episode  249 : Reward =  37.0\n",
      "Episode  250 : Reward =  53.0\n",
      "Episode  251 : Reward =  24.0\n",
      "Episode  252 : Reward =  33.0\n",
      "Episode  253 : Reward =  24.0\n",
      "Episode  254 : Reward =  13.0\n",
      "Episode  255 : Reward =  25.0\n",
      "Episode  256 : Reward =  81.0\n",
      "Episode  257 : Reward =  56.0\n",
      "Episode  258 : Reward =  72.0\n",
      "Episode  259 : Reward =  65.0\n",
      "Episode  260 : Reward =  49.0\n",
      "Episode  261 : Reward =  43.0\n",
      "Episode  262 : Reward =  31.0\n",
      "Episode  263 : Reward =  94.0\n",
      "Episode  264 : Reward =  72.0\n",
      "Episode  265 : Reward =  51.0\n",
      "Episode  266 : Reward =  86.0\n",
      "Episode  267 : Reward =  115.0\n",
      "Episode  268 : Reward =  85.0\n",
      "Episode  269 : Reward =  20.0\n",
      "Episode  270 : Reward =  24.0\n",
      "Episode  271 : Reward =  139.0\n",
      "Episode  272 : Reward =  93.0\n",
      "Episode  273 : Reward =  44.0\n",
      "Episode  274 : Reward =  54.0\n",
      "Episode  275 : Reward =  47.0\n",
      "Episode  276 : Reward =  16.0\n",
      "Episode  277 : Reward =  49.0\n",
      "Episode  278 : Reward =  24.0\n",
      "Episode  279 : Reward =  60.0\n",
      "Episode  280 : Reward =  42.0\n",
      "Episode  281 : Reward =  25.0\n",
      "Episode  282 : Reward =  58.0\n",
      "Episode  283 : Reward =  179.0\n",
      "Episode  284 : Reward =  105.0\n",
      "Episode  285 : Reward =  38.0\n",
      "Episode  286 : Reward =  46.0\n",
      "Episode  287 : Reward =  104.0\n",
      "Episode  288 : Reward =  129.0\n",
      "Episode  289 : Reward =  26.0\n",
      "Episode  290 : Reward =  41.0\n",
      "Episode  291 : Reward =  101.0\n",
      "Episode  292 : Reward =  156.0\n",
      "Episode  293 : Reward =  131.0\n",
      "Episode  294 : Reward =  170.0\n",
      "Episode  295 : Reward =  132.0\n",
      "Episode  296 : Reward =  229.0\n",
      "Episode  297 : Reward =  181.0\n",
      "Episode  298 : Reward =  114.0\n",
      "Episode  299 : Reward =  164.0\n",
      "Episode  300 : Reward =  193.0\n",
      "Episode  301 : Reward =  22.0\n",
      "Episode  302 : Reward =  109.0\n",
      "Episode  303 : Reward =  124.0\n",
      "Episode  304 : Reward =  141.0\n",
      "Episode  305 : Reward =  225.0\n",
      "Episode  306 : Reward =  56.0\n",
      "Episode  307 : Reward =  122.0\n",
      "Episode  308 : Reward =  171.0\n",
      "Episode  309 : Reward =  252.0\n",
      "Episode  310 : Reward =  104.0\n",
      "Episode  311 : Reward =  95.0\n",
      "Episode  312 : Reward =  262.0\n",
      "Episode  313 : Reward =  81.0\n",
      "Episode  314 : Reward =  182.0\n",
      "Episode  315 : Reward =  39.0\n",
      "Episode  316 : Reward =  223.0\n",
      "Episode  317 : Reward =  161.0\n",
      "Episode  318 : Reward =  145.0\n",
      "Episode  319 : Reward =  244.0\n",
      "Episode  320 : Reward =  217.0\n",
      "Episode  321 : Reward =  266.0\n",
      "Episode  322 : Reward =  96.0\n",
      "Episode  323 : Reward =  438.0\n",
      "Episode  324 : Reward =  170.0\n",
      "Episode  325 : Reward =  274.0\n",
      "Episode  326 : Reward =  143.0\n",
      "Episode  327 : Reward =  296.0\n",
      "Episode  328 : Reward =  151.0\n",
      "Episode  329 : Reward =  203.0\n",
      "Episode  330 : Reward =  237.0\n",
      "Episode  331 : Reward =  367.0\n",
      "Episode  332 : Reward =  186.0\n",
      "Episode  333 : Reward =  131.0\n",
      "Episode  334 : Reward =  214.0\n",
      "Episode  335 : Reward =  381.0\n",
      "Episode  336 : Reward =  217.0\n",
      "Episode  337 : Reward =  103.0\n",
      "Episode  338 : Reward =  146.0\n",
      "Episode  339 : Reward =  294.0\n",
      "Episode  340 : Reward =  101.0\n",
      "Episode  341 : Reward =  343.0\n",
      "Episode  342 : Reward =  258.0\n",
      "Episode  343 : Reward =  211.0\n",
      "Episode  344 : Reward =  500.0\n",
      "Episode  345 : Reward =  102.0\n",
      "Episode  346 : Reward =  253.0\n",
      "Episode  347 : Reward =  255.0\n",
      "Episode  348 : Reward =  286.0\n",
      "Episode  349 : Reward =  226.0\n",
      "Episode  350 : Reward =  279.0\n",
      "Episode  351 : Reward =  339.0\n",
      "Episode  352 : Reward =  332.0\n",
      "Episode  353 : Reward =  361.0\n",
      "Episode  354 : Reward =  332.0\n",
      "Episode  355 : Reward =  323.0\n",
      "Episode  356 : Reward =  500.0\n",
      "Episode  357 : Reward =  500.0\n",
      "Episode  358 : Reward =  354.0\n",
      "Episode  359 : Reward =  498.0\n",
      "Episode  360 : Reward =  346.0\n",
      "Episode  361 : Reward =  229.0\n",
      "Episode  362 : Reward =  348.0\n",
      "Episode  363 : Reward =  500.0\n",
      "Episode  364 : Reward =  307.0\n",
      "Episode  365 : Reward =  228.0\n",
      "Episode  366 : Reward =  484.0\n",
      "Episode  367 : Reward =  346.0\n",
      "Episode  368 : Reward =  297.0\n",
      "Episode  369 : Reward =  415.0\n",
      "Episode  370 : Reward =  500.0\n",
      "Episode  371 : Reward =  329.0\n",
      "Episode  372 : Reward =  467.0\n",
      "Episode  373 : Reward =  250.0\n",
      "Episode  374 : Reward =  394.0\n",
      "Episode  375 : Reward =  212.0\n",
      "Episode  376 : Reward =  178.0\n",
      "Episode  377 : Reward =  500.0\n",
      "Episode  378 : Reward =  500.0\n",
      "Episode  379 : Reward =  500.0\n",
      "Episode  380 : Reward =  411.0\n",
      "Episode  381 : Reward =  500.0\n",
      "Episode  382 : Reward =  295.0\n",
      "Episode  383 : Reward =  374.0\n",
      "Episode  384 : Reward =  500.0\n",
      "Episode  385 : Reward =  500.0\n",
      "Episode  386 : Reward =  500.0\n",
      "Episode  387 : Reward =  500.0\n",
      "Episode  388 : Reward =  500.0\n",
      "Episode  389 : Reward =  500.0\n",
      "Episode  390 : Reward =  137.0\n",
      "Episode  391 : Reward =  486.0\n",
      "Episode  392 : Reward =  425.0\n",
      "Episode  393 : Reward =  125.0\n",
      "Episode  394 : Reward =  476.0\n",
      "Episode  395 : Reward =  349.0\n",
      "Episode  396 : Reward =  232.0\n",
      "Episode  397 : Reward =  359.0\n",
      "Episode  398 : Reward =  164.0\n",
      "Episode  399 : Reward =  500.0\n",
      "Episode  400 : Reward =  427.0\n",
      "Episode  401 : Reward =  500.0\n",
      "Episode  402 : Reward =  443.0\n",
      "Episode  403 : Reward =  500.0\n",
      "Episode  404 : Reward =  412.0\n",
      "Episode  405 : Reward =  362.0\n",
      "Episode  406 : Reward =  500.0\n",
      "Episode  407 : Reward =  500.0\n",
      "Episode  408 : Reward =  500.0\n",
      "Episode  409 : Reward =  500.0\n",
      "Episode  410 : Reward =  487.0\n",
      "Episode  411 : Reward =  275.0\n",
      "Episode  412 : Reward =  386.0\n",
      "Episode  413 : Reward =  500.0\n",
      "Episode  414 : Reward =  292.0\n",
      "Episode  415 : Reward =  500.0\n",
      "Episode  416 : Reward =  500.0\n",
      "Episode  417 : Reward =  368.0\n",
      "Episode  418 : Reward =  328.0\n",
      "Episode  419 : Reward =  398.0\n",
      "Episode  420 : Reward =  320.0\n",
      "Episode  421 : Reward =  441.0\n",
      "Episode  422 : Reward =  477.0\n",
      "Episode  423 : Reward =  334.0\n",
      "Episode  424 : Reward =  330.0\n",
      "Episode  425 : Reward =  496.0\n",
      "Episode  426 : Reward =  424.0\n",
      "Episode  427 : Reward =  500.0\n",
      "Episode  428 : Reward =  500.0\n",
      "Episode  429 : Reward =  500.0\n",
      "Episode  430 : Reward =  343.0\n",
      "Episode  431 : Reward =  500.0\n",
      "Episode  432 : Reward =  500.0\n",
      "Episode  433 : Reward =  500.0\n",
      "Episode  434 : Reward =  432.0\n",
      "Episode  435 : Reward =  301.0\n",
      "Episode  436 : Reward =  500.0\n",
      "Episode  437 : Reward =  500.0\n",
      "Episode  438 : Reward =  500.0\n",
      "Episode  439 : Reward =  500.0\n",
      "Episode  440 : Reward =  500.0\n",
      "Episode  441 : Reward =  500.0\n",
      "Episode  442 : Reward =  500.0\n",
      "Episode  443 : Reward =  500.0\n",
      "Episode  444 : Reward =  500.0\n",
      "Episode  445 : Reward =  500.0\n",
      "Episode  446 : Reward =  500.0\n",
      "Episode  447 : Reward =  500.0\n",
      "Episode  448 : Reward =  500.0\n",
      "Episode  449 : Reward =  500.0\n",
      "Episode  450 : Reward =  500.0\n",
      "Episode  451 : Reward =  260.0\n",
      "Episode  452 : Reward =  350.0\n",
      "Episode  453 : Reward =  500.0\n",
      "Episode  454 : Reward =  500.0\n",
      "Episode  455 : Reward =  500.0\n",
      "Episode  456 : Reward =  500.0\n",
      "Episode  457 : Reward =  500.0\n",
      "Episode  458 : Reward =  500.0\n",
      "Episode  459 : Reward =  500.0\n",
      "Episode  460 : Reward =  500.0\n",
      "Episode  461 : Reward =  256.0\n",
      "Episode  462 : Reward =  500.0\n",
      "Episode  463 : Reward =  500.0\n",
      "Episode  464 : Reward =  500.0\n",
      "Episode  465 : Reward =  500.0\n",
      "Episode  466 : Reward =  500.0\n",
      "Episode  467 : Reward =  500.0\n",
      "Episode  468 : Reward =  500.0\n",
      "Episode  469 : Reward =  500.0\n",
      "Episode  470 : Reward =  500.0\n",
      "Episode  471 : Reward =  466.0\n",
      "Episode  472 : Reward =  409.0\n",
      "Episode  473 : Reward =  500.0\n",
      "Episode  474 : Reward =  500.0\n",
      "Episode  475 : Reward =  500.0\n",
      "Episode  476 : Reward =  500.0\n",
      "Episode  477 : Reward =  500.0\n",
      "Episode  478 : Reward =  500.0\n",
      "Episode  479 : Reward =  500.0\n",
      "Episode  480 : Reward =  500.0\n",
      "Episode  481 : Reward =  500.0\n",
      "Episode  482 : Reward =  500.0\n",
      "Episode  483 : Reward =  500.0\n",
      "Episode  484 : Reward =  456.0\n",
      "Episode  485 : Reward =  500.0\n",
      "Episode  486 : Reward =  367.0\n",
      "Episode  487 : Reward =  193.0\n",
      "Episode  488 : Reward =  500.0\n",
      "Episode  489 : Reward =  423.0\n",
      "Episode  490 : Reward =  443.0\n",
      "Episode  491 : Reward =  378.0\n",
      "Episode  492 : Reward =  480.0\n",
      "Episode  493 : Reward =  500.0\n",
      "Episode  494 : Reward =  210.0\n",
      "Episode  495 : Reward =  500.0\n",
      "Episode  496 : Reward =  500.0\n",
      "Episode  497 : Reward =  500.0\n",
      "Episode  498 : Reward =  500.0\n",
      "Episode  499 : Reward =  500.0\n",
      "Episode  500 : Reward =  500.0\n",
      "Episode  501 : Reward =  500.0\n",
      "Episode  502 : Reward =  500.0\n",
      "Episode  503 : Reward =  500.0\n",
      "Episode  504 : Reward =  479.0\n",
      "Episode  505 : Reward =  500.0\n",
      "Episode  506 : Reward =  500.0\n",
      "Episode  507 : Reward =  500.0\n",
      "Episode  508 : Reward =  500.0\n",
      "Episode  509 : Reward =  500.0\n",
      "Episode  510 : Reward =  500.0\n",
      "Episode  511 : Reward =  500.0\n",
      "Episode  512 : Reward =  500.0\n",
      "Episode  513 : Reward =  500.0\n",
      "Episode  514 : Reward =  500.0\n",
      "Episode  515 : Reward =  500.0\n",
      "Episode  516 : Reward =  392.0\n",
      "Episode  517 : Reward =  500.0\n",
      "Episode  518 : Reward =  500.0\n",
      "Episode  519 : Reward =  500.0\n",
      "Episode  520 : Reward =  464.0\n",
      "Episode  521 : Reward =  149.0\n",
      "Episode  522 : Reward =  500.0\n",
      "Episode  523 : Reward =  358.0\n",
      "Episode  524 : Reward =  500.0\n",
      "Episode  525 : Reward =  385.0\n",
      "Episode  526 : Reward =  500.0\n",
      "Episode  527 : Reward =  500.0\n",
      "Episode  528 : Reward =  500.0\n",
      "Episode  529 : Reward =  500.0\n"
     ]
    }
   ],
   "source": [
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 500                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "\n",
    "\n",
    "action_std = None\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "# print_running_reward = 0\n",
    "# print_running_episodes = 0\n",
    "\n",
    "# log_running_reward = 0\n",
    "# log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Check if state is tuple\n",
    "    if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ , _= env.step(action)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    print('Episode ', i_episode, ': Reward = ', current_ep_reward)\n",
    "\n",
    "    # print_running_reward += current_ep_reward\n",
    "    # print_running_episodes += 1\n",
    "\n",
    "    # log_running_reward += current_ep_reward\n",
    "    # log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d804fd-2f4a-4bd4-b5a4-7b1ed0eef647",
   "metadata": {
    "id": "22d804fd-2f4a-4bd4-b5a4-7b1ed0eef647"
   },
   "outputs": [],
   "source": [
    "        # # log in logging file\n",
    "        # if time_step % log_freq == 0:\n",
    "\n",
    "        #     # log average reward till last episode\n",
    "        #     log_avg_reward = log_running_reward / log_running_episodes\n",
    "        #     log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "\n",
    "        #     log_running_reward = 0\n",
    "        #     log_running_episodes = 0\n",
    "\n",
    "        # # printing average reward\n",
    "        # if time_step % print_freq == 0:\n",
    "\n",
    "        #     # print average reward till last episode\n",
    "        #     print_avg_reward = print_running_reward / print_running_episodes\n",
    "        #     print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "        #     print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "        #     print_running_reward = 0\n",
    "        #     print_running_episodes = 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
